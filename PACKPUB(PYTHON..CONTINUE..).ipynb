{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA ANALYTIC WITH PYTHON AND NUMPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Overview\n",
    "By the end of this course, you will be able, use pandas to view, create, analyze, and modify DataFrames; use NumPy to perform statistics and speed up matrix computations; organize and modify data by reading, slicing, and transforming; clean data by deleting or manipulating NaN values and coercing column types; visualize data by constructing, modifying, and interpreting histograms and scatter plots; generate and interpret statistical models using pandas and statsmodels and solve real-world problems using data analytics techniques.\n",
    "\n",
    "Introduction\n",
    "In course, Practical Python – Advanced Topics, you looked at how to use GitHub to collaborate with team members. You also used conda to document and set up the dependencies for Python programs and docker to create reproducible Python environments to run our code.\n",
    "\n",
    "We now shift gears to data science. Data science is booming like never before. Data scientists have become among the most sought-after practitioners in the world today. Most leading corporations have data scientists to analyze and explain their data.\n",
    "\n",
    "Data analytics focuses on the analysis of big data. As each day goes by, there is more data than ever before — far too much for any human to analyze by sight. Leading Python developers such as Wes McKinney and Travis Oliphant addressed the gap by creating specialized Python libraries, in particular, Pandas and NumPy to handle big data.\n",
    "\n",
    "Taken together, Pandas and NumPy are one of the most efficient libraries at handling and transforming the big data. They are built for speed, efficiency, readability, and ease of use.\n",
    "\n",
    "Pandas provide you with a unique framework to view and modify data. Pandas handles all data-related tasks such as creating DataFrames, importing data, scraping data from the web, merging data, pivoting, concatenating, and more.\n",
    "\n",
    "NumPy, short for Numerical Python, is more focused on computation. NumPy interprets the rows and columns of pandas DataFrames in the form of NumPy arrays. When computing descriptive statistics such as the mean, median, mode, and quartiles, NumPy is blazingly fast.\n",
    "\n",
    "Another key player in data analysis is Matplotlib, a graphing library that handles scatter plots, histograms, regression lines, and much more. The importance of data graphs cannot be overstated since most non-technical professionals use them to interpret results.\n",
    "\n",
    "\n",
    "MARK INCOMPLETE\n",
    "\n",
    "CONTINUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumPy and Basic Stats\n",
    "NumPy is designed to handle big data swiftly. It includes the following essential components according to the NumPy documentation:\n",
    "\n",
    "A powerful n-dimensional array object\n",
    "Sophisticated (broadcasting) functions\n",
    "Tools for integrating C/C++ and Fortran code\n",
    "Useful linear algebra, Fourier transform, and random number capabilities\n",
    "You will be using NumPy for the rest of the course. Instead of using lists, you will use NumPy arrays. NumPy arrays are the basic elements of the NumPy package. NumPy arrays are designed to handle arrays of any dimension.\n",
    "\n",
    "Numpy arrays can be indexed easily and can have many types of data, such as float, int, string, and object, but the types must be consistent to improve speed. Unlike list, numpy arrays have all elements of the same data type.\n",
    "\n",
    "Just to compare the speed of numerical operation between numpy and list, let's analyze the following case. We don't need to go in to the details of syntax, we need to understand the computation time of numpy and list:\n",
    "\n",
    "Import the libraries that we will need later and these are numpy and time.\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "Now let's create two variables myArray and myList for a numpy array and a list containing the same number of elements.\n",
    "\n",
    "num_elements = 10000000\n",
    "#create a list and numpy array of 1000000 elements\n",
    "myList = list(range(num_elements))\n",
    "myArray = np.array(range(num_elements))\n",
    "\n",
    "Let's perform a small arithmetical operation of adding 1 to each element of myArray and myList. Also, we can print the time for the operation using the time libarary.\n",
    "\n",
    "#Let's add 1 to each element of list using list comprehension\n",
    "tick = time()\n",
    "myList = [x+1 for x in myList]\n",
    "print(\"Time Taken by the list is {}\".format(time()-tick))\n",
    "\n",
    "It will produce an output. This value can be different for you depending on your system configuration. But it should be around the following number only:\n",
    "\n",
    "Time Taken by the list is 1.7859747409820557\n",
    "\n",
    "#Lets add 1 to each element of numpy array\n",
    "tick = time()\n",
    "myArray =+ 1\n",
    "print(\"Time Taken by the Numpy Array is {}\".format(time()-tick))\n",
    "\n",
    "It should produce output as\n",
    "\n",
    "Time Taken by the Numpy Array is 0.011987686157226562It can be seen that numpy arrays are around 150 times faster than the lists for the same computation which makes numpy arrays the most popular choice for computationally intensive problems like in data science applications. This gap between the execution time further increases as the data size increases. This is a big reason why numpy is so popular in data analytics industries. Let's dive into more details of numpy arrays.\n",
    "\n",
    "Exercise 128: Converting Lists to NumPy Arrays\n",
    "In this exercise, you will convert a list to a numpy array. The following steps will enable you to complete the exercise:\n",
    "\n",
    "Open a new Jupyter Notebook.\n",
    "Firstly, you need to import numpy:\n",
    "import numpy as np\n",
    "Now, you'll create a list for test_scores and confirm the type of data:\n",
    "test_scores = [70,65,95,88]\n",
    "type(test_scores)\n",
    "The output will be as follows:\n",
    "list\n",
    "Note: Now that numpy has been imported, you can access all numpy methods, such as numpy arrays. Type np and then press Tab on your keyboard to see the breadth of options. You are looking for an array.\n",
    "\n",
    "Now, you will convert the list of marks to a numpy array and check the type of the array. Enter the code in the following code snippet:\n",
    "scores = np.array(test_scores)\n",
    "type(scores)\n",
    "The output will be as follows:\n",
    "numpy.ndarray\n",
    "In this exercise, you were able to convert a list of test score marks to a NumPy array. You will find the mean using these values within the NumPy array in Exercise 129, Calculating the Mean of the Test Score.\n",
    "\n",
    "One of the most common statistical measures is the mean. Traditionally thought of as the average, the mean of a list is the sum of each entry divided by the number of entries. In NumPy, the mean may be computed using the .mean method.\n",
    "\n",
    "\n",
    "MARK INCOMPLETE\n",
    "\n",
    "CONTINUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken by the list is0.1034555435180664\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "num_elements = 1000000\n",
    "myList = list(range(num_elements))\n",
    "myArray = np.array(range(num_elements))\n",
    "tick = time()\n",
    "myList = [x+1 for x in myList]\n",
    "print(\"time taken by the list is{}\".format(time()-tick))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken by the Numpy array is0.0\n"
     ]
    }
   ],
   "source": [
    "tick = time()\n",
    "myarray =+1\n",
    "print(\"time taken by the Numpy array is{}\".format(time()-tick))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores = [70,95,65,85]\n",
    "type(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = np.array(test_scores)\n",
    "type(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Calculating the Mean of the Test Score\n",
    "In this exercise, you will use the numpy array you created to store our test scores from Exercise 128, Converting Lists to NumPy Arrays, and you will calculate the mean of testscores. The following steps will enable you to complete the exercise:\n",
    "\n",
    "Continue in the same Jupyter Notebook from Exercise 128, Converting Lists to NumPy Arrays.\n",
    "Now, to find the \"average\" of test_score, you can use the mean method, as shown here:\n",
    "scores.mean()\n",
    "The output will be as follows:\n",
    "79.5\n",
    "Note: The word \"average\" is in quotation marks. This is not an accident. The mean is only one kind of average. Median and Mode are called colloquially as average.\n",
    "\n",
    "Given our test scores of 70, 65, 95, and 88, the \"average\" is 79.5, which is the expected output. In this exercise, you were able to use the mean function of NumPy and find the average of test_scores. In the following exercise, you will find the median using NumPy.\n",
    "\n",
    "The median is the number in the middle when the number of elements in list is odd while it is the mean of two central elements when the number of elements in list is even. Although not necessarily the best measure of test averages, it's an excellent measure of income average. Mean is generally pretty much affected by the outlier data and therefore mean is not a good function when data contains many outliers whereas median is very robust against outliers and produce much better results than mean in such cases.\n",
    "\n",
    "\n",
    "MARK INCOMPLETE\n",
    "\n",
    "CONTINUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.75"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores = [50,47,90,23]\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Finding the Median from a Collection of Income Data\n",
    "In this exercise, you will be finding the median from a collection of income data for a neighborhood and help a millionaire decide whether he should build his dream house in the neighborhood based on the income data. The median function here is a method of numpy.\n",
    "\n",
    "The following steps will enable you to complete the exercise:\n",
    "\n",
    "Open a new Jupyter Notebook.\n",
    "Firstly, you need to import the numpy package as np, then create a numpy array and assign various pieces of income data, as shown in the following code snippet:\n",
    "import numpy as np\n",
    "income = np.array([75000, 55000, 88000, 125000, 64000, 97000])\n",
    "Next, find the mean of the income data:\n",
    "income.mean()\n",
    "The output will be as follows:\n",
    "84000\n",
    "So far, so good. 84000 is the meanincome on your block. Now, say the millionaire decides to build his dream house on the vacant corner lot. He adds a salary of 12 million dollars.\n",
    "Append the value of 12 million dollars to the current array and find the new mean. For appending the new element 12000000 in the income array, we are using the append method:\n",
    "income = np.append(income, 12000000)\n",
    "income.mean()\n",
    "The output will be as follows:\n",
    "1786285.7142857143\n",
    "The new average income is 1.7 million dollars. Okay. Nobody makes close to 1.7 million dollars on the block. It's not a representative average income of the whole area and this will give an incorrect impression of income of the area in general. The mean of data is severely affected due to addition of this outlier salary. This is where the median comes into play.\n",
    "Note: Median here is not a method of np.array class, but it is a method of numpy libarary. (The mean may be computed in the same way, as a method of numpy.)\n",
    "\n",
    "Now to find the median function from the income values you have:\n",
    "np.median(income)\n",
    "The output will be as follows:\n",
    "88000\n",
    "This result says that half of the neighborhood residents make more than 88,000, and half of the blocks make less. This would give the millionaire a fair idea of the neighborhood. In this particular case, the median is a much better estimation of average income than the mean. It is clearly evident that unlike mean, median did not deviate due to the presence of outlier income data from its general estimate.\n",
    "\n",
    "In the next section, you will be covering skewed data and outliers.\n",
    "\n",
    "\n",
    "COMPLETE & CONTINUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55933.333333333336"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "income = np.array([75000,85000,7800])\n",
    "income.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3041950.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income = np.append(income,12000000)\n",
    "income.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Skewed Data and Outliers\n",
    "Something about the 12 million salary does not sit right. It's nowhere near anyone else's income. In statistics, there is official terminology for this. You say that the data is skewed by an outlier of 12,000,000. In particular, the data is right-skewed since 12,000,000 is far to the right of every other data point.\n",
    "\n",
    "Right-skewed data pulls the mean towards the right from the median. In fact, if the mean greatly exceeds the median, this is clear evidence of right-skewed data. Similarly, if the mean is much less than the median, this is clear evidence of left-skewed data which implies that there are some outlier which are extremely low as compared to the rest of the group.\n",
    "\n",
    "Unfortunately, there is no universal way to compute individual outliers. There are some general methods, but you won't get into them here. Just keep in mind that outliers are far away from other data points, and they skew the data distribution.\n",
    "\n",
    "Standard Deviation\n",
    "The standard deviation is a precise statistical measure of how spread out data points are. In the following exercise, you will compute the standard deviation for income data.\n",
    "\n",
    "Exercise 131: Finding the Standard Deviation from Income Data\n",
    "In this exercise, you will be using the income data from Exercise 130, Finding the Median from a Collection of Income Data, and you will find the amount of deviation you have between the income of the millionaire to the regular residents living in the neighborhood.\n",
    "\n",
    "The following steps will enable you to complete the exercise:\n",
    "\n",
    "Continue with the previous Jupyter Notebook.\n",
    "Now check the standard deviation using the std() function, as mentioned in the following code snippet:\n",
    "income.std()\n",
    "The output will be as follows:\n",
    "4169786.007331644\n",
    "As you can see, the standard deviation here is a huge number, which is 4 million, and this could be practically meaningless while you plot the data. On average, the incomes are not 4 million away from mean income\n",
    "Now, try to find the standard deviation from the test_scores data from Exercise 128, Converting Lists to NumPy Arrays.\n",
    "Assign the test_scores list value once again as this is a new Jupyter notebook:\n",
    "test_scores = [70,65,95,88]\n",
    "Now, convert this list to a numpy array:\n",
    "scores = np.array(test_scores)\n",
    "Now, find the standard deviation of test_scores using the std() function:\n",
    "scores.std()\n",
    "The output will be as follows:\n",
    "12.379418403139947\n",
    "In this exercise, you observed that the income data is so skewed that the standard deviation of 4 million is practically meaningless. But the 12.4 standard deviation of the test scores is very meaningful; the mean test score of 79.5 with a standard deviation of 12.4 means that you can expect the scores to be about 12 points away from the mean of the test scores.\n",
    "\n",
    "What if you need to find the maximum, minimum, or sum of the numpy arrays? For instance, you can use the values from test_scores to find the maximum, minimum, and sum of test_scores.\n",
    "\n",
    "You can find the maximum from the numpy array using the max() method, the minimum using the min() method, and the sum using the sum() method.\n",
    "\n",
    "You can easily find the max, the min, and the sum values of numpy arrays.\n",
    "\n",
    "To find the maximum, enter the following code:\n",
    "\n",
    "test_scores = [70,65,95,88]\n",
    "scores = np.array(test_scores)  \n",
    "scores.max()\n",
    "The output will be as follows:\n",
    "\n",
    "95\n",
    "To find the minimum, enter the following code:\n",
    "\n",
    "scores.min()\n",
    "The output will be as follows:\n",
    "\n",
    "65\n",
    "To find the sum, enter the following code:\n",
    "\n",
    "scores.sum()\n",
    "The output will be as follows:\n",
    "\n",
    "318\n",
    "\n",
    "COMPLETE & CONTINUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5172017.779116773"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding standard deveiation form the income data\n",
    "income.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = [70,65,75]\n",
    "scors = np.array(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to find the macimum value\n",
    "test_scores = [75,88,90]\n",
    "scores = np.array(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrices\n",
    "A DataFrame is generally composed of rows, and each row has the same number of columns. From one point of view, it's a two-dimensional grid containing lots of numbers. It can also be interpreted as a list of lists, or an array of arrays.\n",
    "\n",
    "In mathematics, a matrix is a rectangular array of numbers defined by the number of rows and columns. It is standard always to list rows first, and columns second. For instance, a 2 x 3 matrix consists of 2 rows and 3 columns, whereas a 3 x 2 matrix consists of 3 rows and 2 columns.\n",
    "\n",
    "Here is a 4 x 4 matrix:\n",
    "\n",
    "\n",
    "Figure 10.1: Matrix representation of a 4 x 4 matrix\n",
    "Exercise 132: Matrices\n",
    "NumPy has methods for creating matrices or n-dimensional arrays. One option is to place random numbers between 0 and 1 into each entry, as follows.\n",
    "\n",
    "In this exercise, you will implement the various numpy matrix methods and observe the outputs (recall that random.seed will allow us to reproduce the same numbers, and it's okay if you want to generate your own).\n",
    "\n",
    "The following steps will enable you to complete the exercise:\n",
    "\n",
    "Begin with a new Jupyter Notebook.\n",
    "Now, generate a random 5 x 5 matrix, as shown in the following code snippet:\n",
    "import numpy as np \n",
    "np.random.seed(seed=60)\n",
    "random_square = np.random.rand(5,5)\n",
    "random_square\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.2: A random matrix being generated\n",
    "In the preceding code, you have used random.seed. You just invoke random.seed(seed=60), and whenever you run the script, you will get the same sequence of values. This is very similar to the DataFrames that you will be dealing with for the rest of this book. You can go through some code to obtain particular rows, columns, and entries.\n",
    "Now, find the rows and columns of the matrix that is generated. In general, if you omit the columns from the matrix [row, column], numpy will select them all.\n",
    "# First row\n",
    "random_square[0]\n",
    "You should get the following output, consisting of all columns, and the first row:\n",
    "\n",
    "Figure 10.3: The values of the matrix row\n",
    "Note, you can equivalently write the preceding code as:\n",
    "\n",
    "# First row\n",
    "random_square[0, :]\n",
    "Now, to find the values of all the rows, and the first column of the matrix.\n",
    "\n",
    "# First columnrandom_square[:,0]\n",
    "You should get the following output, consisting of all rows, and the first column:\n",
    "\n",
    "\n",
    "Figure 10.4: The values of the matrix column\n",
    "Now find individual entries by specifying the value of the matrix [row, column], as shown in the following code snippet:\n",
    "# First entry\n",
    "random_square[0,0]\n",
    "The output, the entry in the first row and first column, will be as follows:\n",
    "0.30087333004661876\n",
    "Here is the first entry using another way:\n",
    "random_square[0][0]\n",
    "The output will be as follows:\n",
    "0.30087333004661876\n",
    "Entry in third row, fourth column:\n",
    "random_square[2,3]\n",
    "The output will be as follows:\n",
    "0.9924592256795676\n",
    "Now, to find the mean values of the matrix, you will find the mean of the entire matrix, individual rows, and columns using the random_square.mean() method, as shown in the following code snippet.Here is the mean entry of the matrix:\n",
    "random_square.mean()\n",
    "The output will be as follows:\n",
    "0.42917627159618377\n",
    "Here is the mean entry of the first row:\n",
    "random_square[0].mean()\n",
    "The output will be as follows:\n",
    "0.4087444389228477\n",
    "Here is the mean entry of the last column:\n",
    "random_square[:,-1].mean()\n",
    "The output will be as follows:\n",
    "0.35019700684996913\n",
    "In this exercise, you created a random 5 x 5 matrix and implemented a few basic operations on the matrix.\n",
    "\n",
    "\n",
    "COMPLETE & CONTINUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30087333, 0.18694582, 0.32318268, 0.66574957, 0.5669708 ],\n",
       "       [0.39825396, 0.37941492, 0.01058154, 0.1703656 , 0.12339337],\n",
       "       [0.69240128, 0.87444156, 0.3373969 , 0.99245923, 0.13154007],\n",
       "       [0.50032984, 0.28662051, 0.22058485, 0.50208555, 0.63606254],\n",
       "       [0.63567694, 0.08043309, 0.58143375, 0.83919086, 0.29301825]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(seed= 60)\n",
    "random_square = np.random.rand(5,5)\n",
    "random_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30087333, 0.18694582, 0.32318268, 0.66574957, 0.5669708 ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_square[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "random_square[0,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation Time for Large Matrices\n",
    "Now that you have gotten a hang of creating random matrices, you can see how long it takes to generate a large matrix and compute the mean:\n",
    "\n",
    "%%time\n",
    "np.random.seed(seed=60)\n",
    "big_matrix = np.random.rand(100000, 100)\n",
    "The output will be as follows:\n",
    "\n",
    "\n",
    "Figure 10.5: Computation time for a large matrix\n",
    "Now, to find the computation time for the mean of the matrix.\n",
    "\n",
    "%%time\n",
    "big_matrix = np.random.rand(100000, 100)\n",
    "big_matrix.mean()\n",
    "You should get the following output:\n",
    "\n",
    "\n",
    "Figure 10.6: Computation time for the mean of the matrix\n",
    "Your time will be different than ours, but it should be in the order of milliseconds. Not bad. It takes much less than a second to generate a matrix of 10 million entries and compute its mean.\n",
    "\n",
    "In the next exercise, you will create arrays using NumPy and compute various values through them. One such computation you will be using is ndarray. numpy.ndarray is a (usually fixed-size) multidimensional array container of items of the same type and size.\n",
    "\n",
    "\n",
    "COMPLETE & CONTINUE\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "np.random.seed(seed = 60)\n",
    "big_matrix = np.random.rand(100000,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.93 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.499848016200312"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "big_matrix = np.random.rand(1000,100)\n",
    "big_matrix.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Creating an Array to Implement NumPy Computations\n",
    "In this exercise, you will generate a new matrix and perform mathematical operations on it, which will be covered later in this exercise. Unlike with traditional lists, NumPy arrays allow each member of the list to be manipulated with ease. The following steps will enable you to complete the exercise:\n",
    "\n",
    "Open a new Jupyter Notebook.\n",
    "Now import numpy and create ndarray between 1 and 100. Little similar to the range function, np.arange creates a numpy array of numbers starting from the first number to the second number with the third number as increment which 1 in case not specified explicitly in function.\n",
    "import numpy as np\n",
    "np.arange(1, 101)\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.7: Showing ndarray with values 1 to 100\n",
    "Reshape the array to 20 rows and 5 columns using the reshape function. The first argument in reshape represents the number of row while the second argument represent the number of column. It should be noted that the product of the number of rows and the number of columns inside the reshape function should be equal to the number of elements in the numpy array.:\n",
    "np.arange(1, 101).reshape(20,5)\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.8: Output with the reshaped array of 20 rows and 5 columns\n",
    "Now, define mat1 as a 20 x 5 array between 1 and 100 and then subtract 50 from mat1, as shown in the following code snippet:\n",
    "mat1 = np.arange(1, 101).reshape(20,5)\n",
    "mat1 – 50\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.9: Output subtracting values from the array\n",
    "Now, multiply mat1 by 10 and observe the change in the output:\n",
    "mat1 * 10\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.10: Output when you multiply mat1 by 10\n",
    "Now you need to add mat1 to itself, as mentioned in the following code snippet:\n",
    "mat1 + mat1\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.11: Output of adding mat1 to itself\n",
    "Now you will multiply each entry in mat1 by itself. This operation is called squaring the matrix:\n",
    "mat1*mat1\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.12: Output of multiplying mat1 by itself\n",
    "Now, take the dot product of mat1 and mat1.T. It should be noted that for the np.dot for two 1-dimensional arrays (vectors) gives the sum of product of their respective elements. However when two dimensional arrays are used, np.dot gives matrix multiplication.\n",
    "np.dot(mat1, mat1.T)\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.13: Output of the dot product of mat1 and mat1.T\n",
    "Note: The output shown in Figure 10.13 has been truncated.\n",
    "\n",
    "In this exercise, you computed and added in values to an array, after which you implemented different NumPy computations.\n",
    "\n",
    "When it comes to data analysis, NumPy will make your life tremendously easier. The ease with which NumPy arrays may be combined, manipulated, and used to compute standard statistical measures like the mean, median, and standard deviation make them far superior to Python lists. They also handle big data so exceptionally well, it's hard to imagine the world of data science without them.\n",
    "\n",
    "In the next section, you will be covering pandas, which is another library available in Python to make the life of a Python developer much easier.\n",
    "\n",
    "\n",
    "COMPLETE & CONTINUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "        92,  93,  94,  95,  96,  97,  98,  99, 100])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.arange(1,101\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   2,   3,   4,   5],\n",
       "       [  6,   7,   8,   9,  10],\n",
       "       [ 11,  12,  13,  14,  15],\n",
       "       [ 16,  17,  18,  19,  20],\n",
       "       [ 21,  22,  23,  24,  25],\n",
       "       [ 26,  27,  28,  29,  30],\n",
       "       [ 31,  32,  33,  34,  35],\n",
       "       [ 36,  37,  38,  39,  40],\n",
       "       [ 41,  42,  43,  44,  45],\n",
       "       [ 46,  47,  48,  49,  50],\n",
       "       [ 51,  52,  53,  54,  55],\n",
       "       [ 56,  57,  58,  59,  60],\n",
       "       [ 61,  62,  63,  64,  65],\n",
       "       [ 66,  67,  68,  69,  70],\n",
       "       [ 71,  72,  73,  74,  75],\n",
       "       [ 76,  77,  78,  79,  80],\n",
       "       [ 81,  82,  83,  84,  85],\n",
       "       [ 86,  87,  88,  89,  90],\n",
       "       [ 91,  92,  93,  94,  95],\n",
       "       [ 96,  97,  98,  99, 100]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "np.arange(1,101).reshape(20,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-49, -48, -47, -46, -45],\n",
       "       [-44, -43, -42, -41, -40],\n",
       "       [-39, -38, -37, -36, -35],\n",
       "       [-34, -33, -32, -31, -30],\n",
       "       [-29, -28, -27, -26, -25],\n",
       "       [-24, -23, -22, -21, -20],\n",
       "       [-19, -18, -17, -16, -15],\n",
       "       [-14, -13, -12, -11, -10],\n",
       "       [ -9,  -8,  -7,  -6,  -5],\n",
       "       [ -4,  -3,  -2,  -1,   0],\n",
       "       [  1,   2,   3,   4,   5],\n",
       "       [  6,   7,   8,   9,  10],\n",
       "       [ 11,  12,  13,  14,  15],\n",
       "       [ 16,  17,  18,  19,  20],\n",
       "       [ 21,  22,  23,  24,  25],\n",
       "       [ 26,  27,  28,  29,  30],\n",
       "       [ 31,  32,  33,  34,  35],\n",
       "       [ 36,  37,  38,  39,  40],\n",
       "       [ 41,  42,  43,  44,  45],\n",
       "       [ 46,  47,  48,  49,  50]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1 = np.arange(1,101).reshape(20,5)\n",
    "mat1 -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mat1*mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,     4,     9,    16,    25],\n",
       "       [   36,    49,    64,    81,   100],\n",
       "       [  121,   144,   169,   196,   225],\n",
       "       [  256,   289,   324,   361,   400],\n",
       "       [  441,   484,   529,   576,   625],\n",
       "       [  676,   729,   784,   841,   900],\n",
       "       [  961,  1024,  1089,  1156,  1225],\n",
       "       [ 1296,  1369,  1444,  1521,  1600],\n",
       "       [ 1681,  1764,  1849,  1936,  2025],\n",
       "       [ 2116,  2209,  2304,  2401,  2500],\n",
       "       [ 2601,  2704,  2809,  2916,  3025],\n",
       "       [ 3136,  3249,  3364,  3481,  3600],\n",
       "       [ 3721,  3844,  3969,  4096,  4225],\n",
       "       [ 4356,  4489,  4624,  4761,  4900],\n",
       "       [ 5041,  5184,  5329,  5476,  5625],\n",
       "       [ 5776,  5929,  6084,  6241,  6400],\n",
       "       [ 6561,  6724,  6889,  7056,  7225],\n",
       "       [ 7396,  7569,  7744,  7921,  8100],\n",
       "       [ 8281,  8464,  8649,  8836,  9025],\n",
       "       [ 9216,  9409,  9604,  9801, 10000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1*mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   55,   130,   205,   280,   355,   430,   505,   580,   655,\n",
       "          730,   805,   880,   955,  1030,  1105,  1180,  1255,  1330,\n",
       "         1405,  1480],\n",
       "       [  130,   330,   530,   730,   930,  1130,  1330,  1530,  1730,\n",
       "         1930,  2130,  2330,  2530,  2730,  2930,  3130,  3330,  3530,\n",
       "         3730,  3930],\n",
       "       [  205,   530,   855,  1180,  1505,  1830,  2155,  2480,  2805,\n",
       "         3130,  3455,  3780,  4105,  4430,  4755,  5080,  5405,  5730,\n",
       "         6055,  6380],\n",
       "       [  280,   730,  1180,  1630,  2080,  2530,  2980,  3430,  3880,\n",
       "         4330,  4780,  5230,  5680,  6130,  6580,  7030,  7480,  7930,\n",
       "         8380,  8830],\n",
       "       [  355,   930,  1505,  2080,  2655,  3230,  3805,  4380,  4955,\n",
       "         5530,  6105,  6680,  7255,  7830,  8405,  8980,  9555, 10130,\n",
       "        10705, 11280],\n",
       "       [  430,  1130,  1830,  2530,  3230,  3930,  4630,  5330,  6030,\n",
       "         6730,  7430,  8130,  8830,  9530, 10230, 10930, 11630, 12330,\n",
       "        13030, 13730],\n",
       "       [  505,  1330,  2155,  2980,  3805,  4630,  5455,  6280,  7105,\n",
       "         7930,  8755,  9580, 10405, 11230, 12055, 12880, 13705, 14530,\n",
       "        15355, 16180],\n",
       "       [  580,  1530,  2480,  3430,  4380,  5330,  6280,  7230,  8180,\n",
       "         9130, 10080, 11030, 11980, 12930, 13880, 14830, 15780, 16730,\n",
       "        17680, 18630],\n",
       "       [  655,  1730,  2805,  3880,  4955,  6030,  7105,  8180,  9255,\n",
       "        10330, 11405, 12480, 13555, 14630, 15705, 16780, 17855, 18930,\n",
       "        20005, 21080],\n",
       "       [  730,  1930,  3130,  4330,  5530,  6730,  7930,  9130, 10330,\n",
       "        11530, 12730, 13930, 15130, 16330, 17530, 18730, 19930, 21130,\n",
       "        22330, 23530],\n",
       "       [  805,  2130,  3455,  4780,  6105,  7430,  8755, 10080, 11405,\n",
       "        12730, 14055, 15380, 16705, 18030, 19355, 20680, 22005, 23330,\n",
       "        24655, 25980],\n",
       "       [  880,  2330,  3780,  5230,  6680,  8130,  9580, 11030, 12480,\n",
       "        13930, 15380, 16830, 18280, 19730, 21180, 22630, 24080, 25530,\n",
       "        26980, 28430],\n",
       "       [  955,  2530,  4105,  5680,  7255,  8830, 10405, 11980, 13555,\n",
       "        15130, 16705, 18280, 19855, 21430, 23005, 24580, 26155, 27730,\n",
       "        29305, 30880],\n",
       "       [ 1030,  2730,  4430,  6130,  7830,  9530, 11230, 12930, 14630,\n",
       "        16330, 18030, 19730, 21430, 23130, 24830, 26530, 28230, 29930,\n",
       "        31630, 33330],\n",
       "       [ 1105,  2930,  4755,  6580,  8405, 10230, 12055, 13880, 15705,\n",
       "        17530, 19355, 21180, 23005, 24830, 26655, 28480, 30305, 32130,\n",
       "        33955, 35780],\n",
       "       [ 1180,  3130,  5080,  7030,  8980, 10930, 12880, 14830, 16780,\n",
       "        18730, 20680, 22630, 24580, 26530, 28480, 30430, 32380, 34330,\n",
       "        36280, 38230],\n",
       "       [ 1255,  3330,  5405,  7480,  9555, 11630, 13705, 15780, 17855,\n",
       "        19930, 22005, 24080, 26155, 28230, 30305, 32380, 34455, 36530,\n",
       "        38605, 40680],\n",
       "       [ 1330,  3530,  5730,  7930, 10130, 12330, 14530, 16730, 18930,\n",
       "        21130, 23330, 25530, 27730, 29930, 32130, 34330, 36530, 38730,\n",
       "        40930, 43130],\n",
       "       [ 1405,  3730,  6055,  8380, 10705, 13030, 15355, 17680, 20005,\n",
       "        22330, 24655, 26980, 29305, 31630, 33955, 36280, 38605, 40930,\n",
       "        43255, 45580],\n",
       "       [ 1480,  3930,  6380,  8830, 11280, 13730, 16180, 18630, 21080,\n",
       "        23530, 25980, 28430, 30880, 33330, 35780, 38230, 40680, 43130,\n",
       "        45580, 48030]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(mat1,mat1.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The pandas Library\n",
    "Pandas is the Python library that handles data on all fronts. Pandas can import data, read data, and display data in an object called a DataFrame. A DataFrame consists of rows and columns. One way to get a feel for DataFrames is to create one.\n",
    "\n",
    "In the IT industry, pandas is widely used for data manipulation. It is also used for stock prediction, statistics, analytics, big data, and, of course, data science.\n",
    "\n",
    "In the following exercises, you will be working with DataFrames and learning different computations that are available with them.\n",
    "\n",
    "Exercise 134: Using DataFrames to Manipulate Stored Student test score Data\n",
    "In this exercise, you will create a dictionary, which is one of many ways to create a pandas DataFrame. You will then manipulate this data as required. In order to use pandas, you must import pandas, which is universally imported with an alias pd. However you are free to choose the alias. Pandas and NumPy are so omnipresent it's a good idea to import them first before performing any kind of data analysis. The following steps will enable you to complete the exercise:\n",
    "\n",
    "You'll begin by importing pandas as pd:\n",
    "import pandas as pd\n",
    "Now that you have imported pandas, you will create a DataFrame.\n",
    "First, you will create a dictionary of test scores as test_dict:\n",
    "# Create dictionary of test scores\n",
    "test_dict = {'Corey':[63,75,88], 'Kevin':[48,98,92], 'Akshay': [87, 86, 85]}\n",
    "Next, you place the test_dict into the DataFrame using the DataFrame method:\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(test_dict)\n",
    "Now, you can display the Dataframe:\n",
    "# Display DataFrame\n",
    "df\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.14: Output with the values added in the DataFrame\n",
    "You can inspect the DataFrame. First, each dictionary key is listed as a column. Second, the rows are labeled with indices starting with 0 by default. Third, the visual layout is clear and legible.Each column and row of DataFrame is officially represented as a Series. A series is a one-dimensional array. Note that array can be represented both by Series and numpy array, however they are two distinct data types and interconvertible.Now, you will rotate the DataFrame, which is also known as a transpose, a standard pandas method. A transpose turns rows into columns and columns into rows.\n",
    "Copy the code shown in the following code snippet to perform a transpose on the DataFrame:\n",
    "# Transpose DataFrame\n",
    "df = df.T\n",
    "df\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.15: The output of the transpose on the DataFrame\n",
    "You can see that columns and indices are interchanged now. Columns have become 0, 1, and 2. While Corey, Kevin, and Akshay have become indices.\n",
    "\n",
    "In this exercise, you created a DataFrame that holds the values of test scores, and to finish, you transposed this DataFrame to get the output. In the next exercise, you will look at renaming column names and selecting data from the DataFrame, which is an important part of working with pandas.\n",
    "\n",
    "\n",
    "COMPLETE & CONTINUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DataFrame Computations with the Student test score Data\n",
    "In this exercise, you will rename the columns of the DataFrame, and you will then select the data to display as output from the DataFrame:\n",
    "\n",
    "Open a new Jupyter Notebook.\n",
    "Import pandas as pd and enter the student value, as shown in Exercise 134, Using DataFrames to Manipulate Stored Student test score Data. After this, convert it to a DataFrame and have it transposed:\n",
    "import pandas as pd\n",
    "# Create dictionary of test scores\n",
    "test_dict = {'Corey':[63,75,88], 'Kevin':[48,98,92], 'Akshay': [87, 86, 85]}\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(test_dict)\n",
    "df = df.T\n",
    "Now rename the columns to something more precise. You can use .columns on the DataFrame to rename the column names:\n",
    "# Rename Columns\n",
    "df.columns = ['Quiz_1', 'Quiz_2', 'Quiz_3']\n",
    "df\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.16: Output with changed column names\n",
    "Now, select a range of values from specific rows and columns. You will be using .iloc with the index number, which is a function present in a pandas DataFrame for selection using the indices of rows and columns. This is shown in the following step:\n",
    "Select a range of rows:\n",
    "# Access first row by index number\n",
    "df.iloc[0]\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.17: Output with the selected values\n",
    ".iloc general takes two parameters. When only one parameter is specified like in the preceding snippet, it is interpreted as the row index. Another way of doing the same is by providing both the index. The first index represent the row index while the second index represent the column index.\n",
    "# Access first row by index number\n",
    "df.iloc[0,:]\n",
    "This syntax can be read as from df dataframe select row with index 0 and all columns corresponding to that row. Here : as column index represent all the columns.\n",
    "\n",
    "Now, select a column using its name, as shown in the following code snippet.You can access columns by putting the column name in quotes inside of brackets:\n",
    "# Access first column by name\n",
    "df['Quiz_1']\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.18: Output while accessing it with a change in the column name\n",
    "Now, select a column using the dot (.) notation:\n",
    "# Access first column using dot notation\n",
    "df.Quiz_1\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.19: Output of selecting a column using the dot notation\n",
    "Just like in case of selection of the first row where we saw using both row and column indices, similar can be done for selecting a column using its index.\n",
    "\n",
    "# Access first column by its index\n",
    "df.iloc[:, 0]\n",
    "Just like the preceding code, this syntax can be read as from df dataframe select all rows and column with index 0. Here : as row index represent all the rows.\n",
    "\n",
    "Note: There are limitations to using dot notation, so bracket quotations are often preferable.\n",
    "\n",
    "In this exercise, you implemented and changed the column names of the DataFrame. Next, you used .iloc to select data as per our requirement from the DataFrame.\n",
    "\n",
    "In the next exercise, you will implement different computations on the DataFrame.\n",
    "\n",
    "\n",
    "COMPLETE & CONTINUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_dict = {'corey':[63,75,88],'kelvin':[48,23,45],'akshay':[87,90,23]}\n",
    "df = pd.DataFrame(test_dict)\n",
    "df = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quiz_1</th>\n",
       "      <th>Quiz_2</th>\n",
       "      <th>Quiz_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>corey</td>\n",
       "      <td>63</td>\n",
       "      <td>75</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>kelvin</td>\n",
       "      <td>48</td>\n",
       "      <td>23</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>akshay</td>\n",
       "      <td>87</td>\n",
       "      <td>90</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Quiz_1  Quiz_2  Quiz_3\n",
       "corey       63      75      88\n",
       "kelvin      48      23      45\n",
       "akshay      87      90      23"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = ['Quiz_1','Quiz_2','Quiz_3']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Quiz_1    63\n",
       "Quiz_2    75\n",
       "Quiz_3    88\n",
       "Name: corey, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Quiz_1    63\n",
       "Quiz_2    75\n",
       "Quiz_3    88\n",
       "Name: corey, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corey     63\n",
       "kelvin    48\n",
       "akshay    87\n",
       "Name: Quiz_1, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Quiz_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corey     63\n",
       "kelvin    48\n",
       "akshay    87\n",
       "Name: Quiz_1, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Quiz_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corey     63\n",
       "kelvin    48\n",
       "akshay    87\n",
       "Name: Quiz_1, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Computing DataFrames within DataFrames\n",
    "In this exercise, you will use the same test score data and perform more computations on the DataFrame. The following steps will enable you to complete the exercise:\n",
    "\n",
    "Open a new Jupyter Notebook.\n",
    "Import pandas as pd and enter the student value as shown in Exercise 135, DataFrame Computations with the Student testscore Data. After this, convert it to a DataFrame:\n",
    "import pandas as pd\n",
    "# Create dictionary of test scores\n",
    "test_dict = {'Corey':[63,75,88], 'Kevin':[48,98,92], 'Akshay': [87, 86, 85]}\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(test_dict)\n",
    "Now, begin by arranging the rows of the DataFrame, as shown in the following code snippet.You can use the same bracket notation, [], for rows as for lists and strings:\n",
    "# Limit DataFrame to first 2 rows\n",
    "df[0:2]\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.20: Output of arranging rows\n",
    "Transpose the DataFrame:\n",
    "df = df.T\n",
    "df\n",
    "Now, rename the columns to Quiz_1, Quiz_2, and Quiz_3, which was covered in Exercise 135, DataFrame Computations with the Student testscore Data:\n",
    "# Rename Columns\n",
    "df.columns = ['Quiz_1', 'Quiz_2', 'Quiz_3']\n",
    "df\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.21: Output with the renamed column names\n",
    "Now, define a new DataFrame from the first two rows and the last two columns.You can choose the rows and columns by name first, as shown in the following code snippet:\n",
    "# Defining a new DataFrame from first 2 rows and last 2 columns \n",
    "rows = ['Corey', 'Kevin']\n",
    "cols = ['Quiz_2', 'Quiz_3']\n",
    "df_spring = df.loc[rows, cols]\n",
    "df_spring\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.22: Output of the newly defined DataFrame\n",
    "Now, select the first two rows and the last two columns using index numbers.You can use .iloc to select rows and columns by index, as shown in the following code snippet:\n",
    "# Select first 2 rows and last 2 columns using index numbers \n",
    "df.iloc[[0,1], [1,2]]Alternatively, you can achieve the preceding task using the following syntax as well.\n",
    "# Select first 2 rows and last 2 columns using index numbers \n",
    "df.iloc[0:2, 1:3]\n",
    "This syntax is particularly useful when the number of columns selected is too large to mention in the form of a list. You should get the following output:\n",
    "\n",
    "\n",
    "Figure 10.23: Output of selecting the first two rows and last two columns using index numbers\n",
    "Now, add a new column to find the quiz average of our students.\n",
    "\n",
    "You can generate new columns in a variety of ways. One way is to use available methods such as the mean. In pandas, it's important to specify the axis. An axis of 0 represents the index, and an axis of 1 represents the rows.\n",
    "\n",
    "Now, create a new column as the mean, as shown in the following code snippet:\n",
    "# Define new column as mean of other columns\n",
    "df['Quiz_Avg'] = df.mean(axis=1)\n",
    "df\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.24: Adding a new quiz_avg column to the output\n",
    "A new column can also be added as a list by choosing the rows and columns by name first.\n",
    "Create a new column as a list, as shown in the following code snippet:\n",
    "df['Quiz_4'] = [92, 95, 88]\n",
    "df\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.25: Output with a newly added column using lists\n",
    "What if you need to delete the column you created? You can do so by using the del function. It's easy to delete columns in pandas using del.\n",
    "Now, delete the Quiz_Avg column as it is not needed anymore:\n",
    "del df['Quiz_Avg']\n",
    "df\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.26: Output with a deleted column\n",
    "In this exercise, you implemented different ways to add and remove columns as per our requirements. In the next section, you will be looking at new rows and NaN, which is an official numpy term.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_dict = {'corey':[63,75,88],'kelvin':[23,45,67],'akskay':[89,34,23]}\n",
    "df = pd.DataFrame(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corey</th>\n",
       "      <th>kelvin</th>\n",
       "      <th>akskay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>23</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>45</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>88</td>\n",
       "      <td>67</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   corey  kelvin  akskay\n",
       "0     63      23      89\n",
       "1     75      45      34\n",
       "2     88      67      23"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transpose  the DataFrame\n",
    "df = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>corey</td>\n",
       "      <td>63</td>\n",
       "      <td>75</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>kelvin</td>\n",
       "      <td>23</td>\n",
       "      <td>45</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>akskay</td>\n",
       "      <td>89</td>\n",
       "      <td>34</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0   1   2\n",
       "corey   63  75  88\n",
       "kelvin  23  45  67\n",
       "akskay  89  34  23"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['quiz_1', 'quiz_2'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-33cdbdd82a03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'corey'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'kelvin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'quiz_1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'quiz_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_spring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1416\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1419\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[1;31m# ugly hack for GH #836\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_multi_take_opportunity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_multi_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         \u001b[1;31m# no shortcut needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_multi_take\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    867\u001b[0m         d = {\n\u001b[0;32m    868\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 869\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AXIS_ORDERS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m         }\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_with_indexers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    867\u001b[0m         d = {\n\u001b[0;32m    868\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 869\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AXIS_ORDERS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m         }\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_with_indexers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m         self._validate_read_indexer(\n\u001b[1;32m-> 1092\u001b[1;33m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1093\u001b[0m         )\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1175\u001b[0m                 raise KeyError(\n\u001b[0;32m   1176\u001b[0m                     \"None of [{key}] are in the [{axis}]\".format(\n\u001b[1;32m-> 1177\u001b[1;33m                         \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m                     )\n\u001b[0;32m   1179\u001b[0m                 )\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['quiz_1', 'quiz_2'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "rows = ['corey','kelvin']\n",
    "cols = ['quiz_1','quiz_2']\n",
    "df_spring = df.loc[rows,cols]\n",
    "df.spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>corey</td>\n",
       "      <td>75</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>kelvin</td>\n",
       "      <td>45</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         1   2\n",
       "corey   75  88\n",
       "kelvin  45  67"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0:2,1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>Quiz_Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>corey</td>\n",
       "      <td>63</td>\n",
       "      <td>75</td>\n",
       "      <td>88</td>\n",
       "      <td>75.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>kelvin</td>\n",
       "      <td>23</td>\n",
       "      <td>45</td>\n",
       "      <td>67</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>akskay</td>\n",
       "      <td>89</td>\n",
       "      <td>34</td>\n",
       "      <td>23</td>\n",
       "      <td>48.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0   1   2   Quiz_Avg\n",
       "corey   63  75  88  75.333333\n",
       "kelvin  23  45  67  45.000000\n",
       "akskay  89  34  23  48.666667"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Quiz_Avg'] = df.mean(axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>Quiz_Avg</th>\n",
       "      <th>Quiz_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>corey</td>\n",
       "      <td>63</td>\n",
       "      <td>75</td>\n",
       "      <td>88</td>\n",
       "      <td>75.333333</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>kelvin</td>\n",
       "      <td>23</td>\n",
       "      <td>45</td>\n",
       "      <td>67</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>akskay</td>\n",
       "      <td>89</td>\n",
       "      <td>34</td>\n",
       "      <td>23</td>\n",
       "      <td>48.666667</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0   1   2   Quiz_Avg  Quiz_4\n",
       "corey   63  75  88  75.333333      92\n",
       "kelvin  23  45  67  45.000000      34\n",
       "akskay  89  34  23  48.666667      56"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Quiz_4'] = [92,34,56]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>Quiz_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>corey</td>\n",
       "      <td>63</td>\n",
       "      <td>75</td>\n",
       "      <td>88</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>kelvin</td>\n",
       "      <td>23</td>\n",
       "      <td>45</td>\n",
       "      <td>67</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>akskay</td>\n",
       "      <td>89</td>\n",
       "      <td>34</td>\n",
       "      <td>23</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0   1   2  Quiz_4\n",
       "corey   63  75  88      92\n",
       "kelvin  23  45  67      34\n",
       "akskay  89  34  23      56"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df['Quiz_Avg']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New Rows and NaN\n",
    "It's not easy to add new rows to a pandas DataFrame. A common strategy is to generate a new DataFrame and then to concatenate the values.\n",
    "\n",
    "Say you have a new student who joins the class for the fourth quiz. What values should you put for the other three quizzes? The answer is NaN. It stands for Not a Number\n",
    "\n",
    "NaN is an official NumPy term. It can be accessed using np.NaN. It is case-sensitive. In later exercises, you will look at how NaN can be used. In the next exercise, you will look at concatenating and working with null values.\n",
    "\n",
    "Exercise 137: Concatenating and Finding the Mean with Null Values for Our test score Data\n",
    "In this exercise, you will be concatenating and finding the mean with null values for the student test score data you created in Exercise 136, Computing DataFrames within DataFrames with four quiz scores. The following steps will enable you to complete the exercise:\n",
    "\n",
    "Open a new Jupyter Notebook.\n",
    "Import pandas and numpy and create a dictionary with the testscore data to be transformed into a DataFrame, as mentioned in Exercise 134, Using DataFrames to Manipulate Stored Student test score Data:\n",
    "import pandas as pd\n",
    "# Create dictionary of test scores\n",
    "test_dict = {'Corey':[63,75,88], 'Kevin':[48,98,92], 'Akshay': [87, 86, 85]}\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(test_dict)\n",
    "Transpose the DataFrame and rename the columns:\n",
    "df = df.T\n",
    "df\n",
    "# Rename Columns\n",
    "df.columns = ['Quiz_1', 'Quiz_2', 'Quiz_3']\n",
    "df\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.27: The transposed and renamed DataFrame\n",
    "Now, add a new column, as shown in Exercise 136, Computing DataFrames within DataFrames:\n",
    "df['Quiz_4'] = [92, 95, 88]\n",
    "df\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.28: Output of the added Quiz_4 column\n",
    "Now, add a new row with the value set as Adrian, as shown in the following code snippet:\n",
    "import numpy as np\n",
    "# Create new DataFrame of one row\n",
    "df_new = pd.DataFrame({'Quiz_1':[np.NaN], 'Quiz_2':[np.NaN], 'Quiz_3': [np.NaN], 'Quiz_4':[71]}, index=['Adrian'])\n",
    "Now, concatenate Dataframe with the added new row, Adrian, and display the new Dataframe value using df:\n",
    "# Concatenate DataFrames\n",
    "df = pd.concat([df, df_new])\n",
    "# Display new DataFrame\n",
    "df\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.29: Output with the row added to the DataFrame\n",
    "You can now compute the new mean, but you must skip the NaN values; otherwise, there will be no mean score for Adrian. This will be fixed in step 7.\n",
    "Find the mean value ignoring NaN and use these values to create a new column named Quiz-Avg, as shown in the following code snippet:\n",
    "df['Quiz_Avg'] = df.mean(axis=1, skipna=True)\n",
    "df\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.30: Output with quiz_4 values added to Adrian\n",
    "Notice that all values are floats except for Quiz_4. There will be occasions when you need to cast all values in a particular column as another type.\n",
    "\n",
    "\n",
    "COMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_dict = {'corey':[63,90,28],'kelvin':[48,90,34],'aksay':[87,23,89]}\n",
    "df = pd.DataFrame(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quiz_1</th>\n",
       "      <th>Quiz_2</th>\n",
       "      <th>Quiz_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>corey</td>\n",
       "      <td>63</td>\n",
       "      <td>90</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>kelvin</td>\n",
       "      <td>48</td>\n",
       "      <td>90</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aksay</td>\n",
       "      <td>87</td>\n",
       "      <td>23</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Quiz_1  Quiz_2  Quiz_3\n",
       "corey       63      90      28\n",
       "kelvin      48      90      34\n",
       "aksay       87      23      89"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transpose the dataframe and rename the column\n",
    "df = df.T\n",
    "df\n",
    "df.columns = ['Quiz_1','Quiz_2','Quiz_3']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quiz_1</th>\n",
       "      <th>Quiz_2</th>\n",
       "      <th>Quiz_3</th>\n",
       "      <th>Quiz_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>corey</td>\n",
       "      <td>63</td>\n",
       "      <td>90</td>\n",
       "      <td>28</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>kelvin</td>\n",
       "      <td>48</td>\n",
       "      <td>90</td>\n",
       "      <td>34</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aksay</td>\n",
       "      <td>87</td>\n",
       "      <td>23</td>\n",
       "      <td>89</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Quiz_1  Quiz_2  Quiz_3  Quiz_4\n",
       "corey       63      90      28      90\n",
       "kelvin      48      90      34      23\n",
       "aksay       87      23      89      45"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Quiz_4'] = [90,23,45]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#adding new row with the set value as adrin as ahown in following snippet\n",
    "#create a new DataFrame  of one row\n",
    "df_new = pd.DataFrame({'Quiz_1':[np.NaN],'Quiz_2':[np.NaN],'Quiz_3':[np.NaN],'Quiz_4':[66]}\n",
    "                     ,index = ['Adrian'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quiz_1</th>\n",
       "      <th>Quiz_2</th>\n",
       "      <th>Quiz_3</th>\n",
       "      <th>Quiz_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>corey</td>\n",
       "      <td>63.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>kelvin</td>\n",
       "      <td>48.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aksay</td>\n",
       "      <td>87.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Adrian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Quiz_1  Quiz_2  Quiz_3  Quiz_4\n",
       "corey     63.0    90.0    28.0      90\n",
       "kelvin    48.0    90.0    34.0      23\n",
       "aksay     87.0    23.0    89.0      45\n",
       "Adrian     NaN     NaN     NaN      66"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df,df_new])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quiz_1</th>\n",
       "      <th>Quiz_2</th>\n",
       "      <th>Quiz_3</th>\n",
       "      <th>Quiz_4</th>\n",
       "      <th>Quiz_Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>corey</td>\n",
       "      <td>63.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>90</td>\n",
       "      <td>67.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>kelvin</td>\n",
       "      <td>48.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>23</td>\n",
       "      <td>48.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aksay</td>\n",
       "      <td>87.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>45</td>\n",
       "      <td>61.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Adrian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66</td>\n",
       "      <td>66.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Quiz_1  Quiz_2  Quiz_3  Quiz_4  Quiz_Avg\n",
       "corey     63.0    90.0    28.0      90     67.75\n",
       "kelvin    48.0    90.0    34.0      23     48.75\n",
       "aksay     87.0    23.0    89.0      45     61.00\n",
       "Adrian     NaN     NaN     NaN      66     66.00"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Quiz_Avg'] = df.mean(axis=1,skipna = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cast Column Types\n",
    "For the sake of consistency, you will do that here. Cast all the int data type in Quiz_4 that you used in Exercise 137, Concatenating and Finding the Mean with Null Values for Our test score Data as floatusing the following code snippet:\n",
    "\n",
    "df.Quiz_4.astype(float)\n",
    "You should get the following output:\n",
    "\n",
    "\n",
    "Figure 10.31: Output with the cast column types\n",
    "You can observe the change in values by examining the DataFrame yourself. Now, move on to the next topic: which is data.\n",
    "\n",
    "\n",
    "COMPLETE & CONTINUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corey     90.0\n",
       "kelvin    23.0\n",
       "aksay     45.0\n",
       "Adrian    66.0\n",
       "Name: Quiz_4, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Quiz_4.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data\n",
    "Now that you have been introduced to NumPy and pandas, you will use them to analyze some real data.\n",
    "\n",
    "Data scientists analyze data that exists in the cloud or online. One strategy is to download data directly to your computer.\n",
    "\n",
    "Note: It is recommended to create a new folder to store all of your data. You can open your Jupyter Notebook in this same folder.\n",
    "\n",
    "Downloading Data\n",
    "Data comes in many formats, and pandas is equipped to handle most of them. In general, when looking for data to analyze, it's worth searching the keyword \"dataset.\" A dataset is a collection of data. Online, \"data\" is everywhere, whereas datasets contain data in its raw format.\n",
    "\n",
    "You will start by examining the famous Boston Housing dataset from 1980, which is available on our GitHub repository.\n",
    "\n",
    "This dataset can be found here.\n",
    "\n",
    "You can begin by first downloading the dataset onto our system.\n",
    "\n",
    "Downloading the Boston Housing Data from GitHub\n",
    "Head to the GitHub repository and download the dataset onto your system.\n",
    "Move the downloaded dataset file into your data folder.\n",
    "Open a Jupyter Notebook in the same folder.\n",
    "Reading Data\n",
    "Now that the data is downloaded, and the Jupyter Notebook is open, you are ready to read the file. The most important part of reading a file is the extension. Our file is a .csv file. You need a method for reading .csv files.\n",
    "\n",
    "CSV stands for Comma-Separated Values. CSV files are a popular way of storing and retrieving data, and pandas handles them very well.\n",
    "\n",
    "Here is a list of standard data files that pandas will read, along with the code for reading data:\n",
    "\n",
    "\n",
    "Figure 10.32: Standard data files that pandas read\n",
    "If the files are clean, pandas will read them properly. Sometimes, files are not clean, and changing function parameters may be required. It's advisable to copy any errors and search for solutions online.\n",
    "\n",
    "A further point of consideration is that the data should be read into a DataFrame. Pandas will convert the data into a DataFrame upon reading it, but you need to save DataFrame in a variable.\n",
    "\n",
    "In Exercise 138, Reading and Viewing the Boston Housing Dataset, you will be using the Boston Housing dataset and performing basic actions on the data.\n",
    "\n",
    "\n",
    "COMPLETE & CONTINUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/PacktWorkshops/The-Python-Workshop/blob/master/Datasets/HousingData.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reading and Viewing the Boston Housing Dataset\n",
    "In this exercise, your goal is to read and view the Boston Housing dataset in our Jupyter Notebook. The following steps will enable you to complete the exercise:\n",
    "\n",
    "Open a new Jupyter Notebook.\n",
    "Import pandas as pd:\n",
    "import pandas as pd\n",
    "Now, choose a variable to store DataFrame and place the HousingData.csv file in the folder for Exercise 138. Then, run the following command:\n",
    "housing_df = pd.read_csv('HousingData.csv')\n",
    "If no errors arise, the file has been properly read. Now you can examine and view the file.\n",
    "Now, you need to view the file by entering the following command:\n",
    "housing_df.head()\n",
    "The pandas .head method does just that. By default, it selects the first five rows. You may enter more if you choose by adding a number in parentheses. You should get the following output:\n",
    "\n",
    "Figure 10.33: Output letting us view the first five rows of the dataset\n",
    "Before you explore operations performed on this dataset, you may wonder what values such as CRIM and ZN mean in the dataset.\n",
    "\n",
    "It's always nice to view data to get a general feel for things. In this particular case, you might want to know what the columns actually mean:\n",
    "\n",
    "\n",
    "Figure 10.34: Representation of the column values of the dataset\n",
    "Now that you know what the values in the dataset mean you will start performing some advanced operations on the dataset. You will cover this in the following exercise.\n",
    "\n",
    "\n",
    "COMPLETE & CONTINUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gaining Data Insights on the Boston Housing Dataset\n",
    "In this exercise, you will be performing some more advanced operations and using pandas methods to understand the dataset and get desired insights. The following steps will enable you to complete the exercise:\n",
    "\n",
    "Open a new Jupyter Notebook and copy the dataset file into a separate folder where you will perform this exercise.\n",
    "Import pandas and choose a variable to store DataFrame and place the HousingData.csv file:\n",
    "import pandas as pd\n",
    "housing_df = pd.read_csv('HousingData.csv')\n",
    "Now, use the describe() method to display key statistical measures of each column, including the mean, median, and quartiles, as shown in the following code snippet:\n",
    "housing_df.describe()\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.35: Output with the desired () method\n",
    "In this output, you will review the meaning of each row.Count: The number of rows with actual values.Mean: The sum of each entry divided by the number of entries. It is often a good estimate of the average.Std: The number of unit entries that are expected to deviate from the mean. It is a good measure of spread.Min: The smallest entry in each column.25%: The first quartile. 25% of the data has a value less than this number.50%: The median. The halfway marker of the data. It is another good estimate of the average.75%: The third quartile. 75% of the data has a value less than this number.Max: The largest entry in each column.\n",
    "Now use the info() method to deliver a full list of columns.info() is especially valuable when you have hundreds of columns, and it takes a long time to horizontally scroll through each one:\n",
    "housing_df.info()\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.36: Output with the info () method\n",
    "As you can see, .info() reveals the count of non-null values in each column along with the column type. Since some columns have less than 506 non-null values, it's safe to assume that the other values are null values.In this dataset, there are a total of 506 rows and 14 columns. You can use the .shape attribute to obtain this information.Now confirm the number of rows and columns in the dataset:\n",
    "housing_df.shape\n",
    "You should get the following output:\n",
    "(506, 14)\n",
    "This confirms that you have 506 rows and 14 columns. Notice that shape does not have any parentheses after it. This is because it's technically an attribute and pre-computed.\n",
    "In this exercise, you performed basic operations on the dataset, such as describing the dataset and finding the number of rows and columns in the dataset.\n",
    "\n",
    "In the next section, you will cover null values.\n",
    "\n",
    "\n",
    "MARK INCOMPLETE\n",
    "\n",
    "CONTINUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Null Values\n",
    "You need to do something about the null values. There are several popular choices when dealing with null values:\n",
    "\n",
    "Eliminate the rows: A great approach if null values are a very small percentage, such as 1% of the total dataset. However, if data is limited, it is not wise to lose the precious data.\n",
    "Replace with a significant value, such as the median or the mean: A great approach if the rows are valuable, and the column is reasonably balanced. However, all the data with missing values are inputted with the same value which is practically not true.\n",
    "Replace with the most likely value, perhaps a 0 or 1: It's preferable to option 2 when the median might be useless. The median can often work here.\n",
    "Other advance techniques include estimating the data using machine learning based model such as KNN, Random Forest, and so on, which is out of the scope of this book.\n",
    "This process of filling the null values with an appropriate number is known as imputation of the data.\n",
    "Note: mode is the official term for the value that occurs the greatest number of times.\n",
    "\n",
    "As you can see, which option you choose depends on the data.\n",
    "\n",
    "Exercise 140: Null Value Operations on the Dataset\n",
    "In this exercise, you will perform a null value operation. You can only select the columns that have null values in our dataset:\n",
    "\n",
    "Open a new Jupyter Notebook and copy the dataset file within a separate folder where you will perform this exercise.\n",
    "Import pandas and choose a variable to store the DataFrame and place the HousingData.csv file:\n",
    "import pandas as pd\n",
    "housing_df = pd.read_csv('HousingData.csv')\n",
    "Now, find values and columns in the dataset with null values, as shown in the following code snippet:\n",
    "housing_df.isnull().any()\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.37: Output of the columns with null values\n",
    "The .isnull() method will display an entire DataFrame of True/False values depending on the Null value. Give it a try. The .any() method returns the individual columns.Take it a step further and choose the DataFrame with those columns.\n",
    "Now, using the DataFrame, find the null columns.You can use .loc to find the location of particular rows. You will select the first five rows and all of the columns that have null values, as shown in the following code snippet:\n",
    "housing_df.loc[:5, housing_df.isnull().any()]\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.38: Output of the dataset with the null columns\n",
    "Now for the final step. Use the .describe method on these particular columns but select all of the rows.\n",
    "Use the .describe() method on the columns with null data of the dataset.The code mentioned ahead is a long piece of the code snippet. housing_df is the DataFrame. .loc allows you to specify rows and columns. : selects all rows. housing_df.isnull().any() selects only columns with null values. .describe() pulls up the statistics:\n",
    "housing_df.loc[:, housing_df.isnull().any()].describe()\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.39: Output with the .describe() method on particular columns but instead selects all of the rows\n",
    "Consider the first column, CRIM. The mean is way more than the median (50%). This indicates that the data is very right-skewed with some outliers. Indeed, you can see that the maximum of 88.97 is much larger than the 3.56 value of the 75th percentile. This makes the mean a poor replacement candidate for this column.\n",
    "\n",
    "It turns out, after examining every column, that the median is a good candidate. Although the median is clearly not better than the mean in some cases, there are a few cases where the mean is clearly worse (CRIM, ZN, and CHAS).\n",
    "\n",
    "The choice depends on what you ultimately want to do with the data. If the goal is a straightforward data analysis, eliminating the rows with null values is worth consideration. However, if the goal is to use machine learning to predict data, then perhaps more is to be gained by changing the null values to a suitable replacement. It's impossible to know in advance.\n",
    "\n",
    "A more thorough examination could be warranted depending on the data. For instance, if analyzing new medical drugs, it would be worth putting more time and energy into appropriately dealing with null values. You may want to perform more analysis to determine whether a value is 0 or 1, depending upon other factors.\n",
    "\n",
    "In this particular case, replacing all the null values with the median is warranted. You can have a look at this in the following example.\n",
    "\n",
    "\n",
    "COMPLETE & CONTINUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Replacing Null Values\n",
    "Pandas include a nice method, fillna, which can be used to replace null values. It works for individual columns and entire DataFrames. You will use three approaches, replacing the null values of a column with the mean, replacing the null values of a column with another value, and replacing all the null values in the entire dataset with the median. You will use the same Boston Housing dataset.\n",
    "\n",
    "Replace null column values with mean:\n",
    "\n",
    "housing_df['AGE'] = housing_df['AGE'].fillna(housing_df.mean())\n",
    "Then, replace null column values with a value (0 in this case):\n",
    "\n",
    "housing_df['CHAS'] = housing_df['CHAS'].fillna(0)\n",
    "Replace null DataFrame values with median:\n",
    "\n",
    "housing_df = housing_df.fillna(housing_df.median())\n",
    "Finally, check that all null values have been replaced:\n",
    "\n",
    "housing_df.info()\n",
    "You should get the following output:\n",
    "\n",
    "\n",
    "Figure 10.40: Output with the null values eliminated\n",
    "After eliminating all null values, the dataset is much cleaner. There may also be unrealistic outliers or extreme outliers that will lead to poor prediction. These can often be detected through visual analysis, which you will be covering in the next section.\n",
    "\n",
    "\n",
    "COMPLETE & CONTINUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visual Analysis\n",
    "Most people interpret the data visually. They prefer to view colorful, meaningful graphs that make sense of the data. As a data science practitioner, it's your job to create these graphs.\n",
    "\n",
    "In this section, you will primarily focus on two kinds of graphs: histograms and scatter plots. You will use Python to create these graphs. Although software packages such as Tableau are rather popular, they are essentially drag and drop. Since Python is an all-purpose programming language, the limitations are only what you know and are capable of doing.\n",
    "\n",
    "The matplotlib Library\n",
    "A popular Python library for creating graphs is matplotlib. It's traditionally imported as plt, as shown in the following code snippet:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "Note the strange second line of code. It basically shows all graphs within Jupyter Notebooks instead of exporting them to external files. It's used when you want to see the graphs right there in front of you.\n",
    "\n",
    "Histograms\n",
    "Creating a histogram is rather simple. You choose a column and place it inside of plt.hist(). The general idea behind a histogram is that it groups the x value (in our case, the median home value) into various bins. The height of the bin is determined by the number of values that fall into that particular range. By default, Matplotlib selects 10 bins.\n",
    "\n",
    "In order to make the graph more meaningful, you should add some labels. You can also use the Seaborn library by importing seaborn as sns , which provides some nice added visuals.\n",
    "\n",
    "\n",
    "COMPLETE & CONTINUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Creating a Histogram Using the Boston Housing Dataset\n",
    "In this exercise, you will use MEDV, the median value of the Boston Homes dataset, a future target column for machine learning. The following steps will enable you to complete the exercise:\n",
    "\n",
    "Open a new Jupyter Notebook and copy the dataset file into a separate folder where you will perform this exercise.\n",
    "Import pandas as pd and choose a variable, housing_df, to store the DataFrame after reading the HousingData csv file.:\n",
    "import pandas as pd\n",
    "housing_df = pd.read_csv('HousingData.csv')\n",
    "Next, you need to import matplotlib as plt, as shown here:\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "Note: The %matplotlib inline ensures that your graphs appear in the Jupyter Notebook instead of external files.\n",
    "\n",
    "Now, import seaborn which will make the graph more visually appealing.\n",
    "import seaborn as sns\n",
    "# Set up seaborn dark grid\n",
    "sns.set()\n",
    "Now plot the histogram for the dataset.\n",
    "plt.hist(housing_df['MEDV'])\n",
    "plt.show()\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.41: Output of the histogram\n",
    "Now, plot the changes onto the histogram, as shown in the following code snippet:\n",
    "plt.hist(housing_df['MEDV'])\n",
    "plt.title('Median Boston Housing Prices')\n",
    "plt.xlabel('1980 Median Value in Thousands')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.42: Histogram with meaningful values using seaborn\n",
    "Now, this is a bit clearer. But in the Jupyter Notebook, the entry is a little small. You can make the graph and the title larger. You can also save the figure using the title of the graph.\n",
    "Make the histogram clearer by increasing the dpi(dots per inch) and the fontsize. This indicates that the output is really flexible, as per our needs. Copy the following code snippet and observe the change in the output:\n",
    "title = 'Median Boston Housing Prices'\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(housing_df['MEDV'])\n",
    "plt.title(title, fontsize=15)\n",
    "plt.xlabel('1980 Median Value in Thousands')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig(title, dpi=300)\n",
    "plt.show()\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.43: Output of an ideal histogram\n",
    "By the end of this exercise, you were able to achieve a solid histogram, which can help to communicate your results to a wider audience.\n",
    "\n",
    "Now, say you want to create another histogram. Should you keep copying the same code? Copying code repeatedly is never a good idea. It's better to write functions.\n",
    "\n",
    "\n",
    "COMPLETE & CONTINUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Histogram Functions\n",
    "Define a histogram function, as mentioned in the following code snippet:\n",
    "\n",
    "def my_hist(column, title, xlab, ylab):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.hist(column)\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.savefig(title, dpi=300)\n",
    "    plt.show()\n",
    "It's not easy to create functions with Matplotlib, so you should go over the parameters carefully. The figsize allows you to establish the size of the figure. The column is the essential parameter. It's what you are going to be graphing. Next, you have the title, followed by the label for the x and y axes. Finally, you save the figure and show the plot. Inside of the function is basically the same code that you ran before. You can try it on a new column — the number of rooms.\n",
    "\n",
    "Do this by calling the histogram function, as mentioned in the following code snippet:\n",
    "\n",
    "my_hist(housing_df['RM'], 'Average Number of Rooms in Boston Households', 'Average Number of Rooms', 'Count')\n",
    "You should get the following output:\n",
    "\n",
    "\n",
    "Figure 10.44: Output with the histogram function\n",
    "Okay. It's not bad, but there's one glaring issue. It's the distribution of bins. It seems most rooms have an average of six, but how many of those are closer to seven? Our graph could be improved if each histogram was clearly between two numbers on the plot. A survey of the preceding data (check max and min from .describe()), reveals that the average number of rooms is between three and nine.\n",
    "\n",
    "In addition to changing the bins, you can also add options to change the color, and the alpha, or transparency.\n",
    "\n",
    "You can improve the built histogram function, as mentioned in the following code snippet:\n",
    "\n",
    "def my_hist(column, title, xlab, ylab, bins=10, alpha=0.7, color='c'):\n",
    "    title = title\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.hist(column, bins=bins, range=(3,9), alpha=alpha, color=color)\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.savefig(title, dpi=300)\n",
    "    plt.show() \n",
    "The number of bins is 10 by default, so you can change it whenever you desire. alpha is a convenient tool that can be used to make the bars transparent. 1.0 has no transparency, and 0.0 has full transparency. Play around with this to select the number that you like.\n",
    "\n",
    "Call the improved histogram function:\n",
    "\n",
    "my_hist(housing_df['RM'], 'Average Number of Rooms in Boston', \n",
    "'Average Number of Rooms', 'Count', bins=6)\n",
    "You should get the following output:\n",
    "\n",
    "\n",
    "Figure 10.45: Output of the improved histogram function being called\n",
    "Now it's clear that the highest average is between six and seven. Also, note that this dataset is surveying groups of houses, not individual houses. This is why the average number of rooms is between 6 and 7.\n",
    "\n",
    "Now that you have understood histograms you can have a look at another type of visual data analysis, called scatter plots.\n",
    "\n",
    "\n",
    "COMPLETE & CONTINUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scatter Plots\n",
    "Scatter plots are hugely important in data analysis. They require an x value, and a y value, typically taken from two numeric columns of a DataFrame. You can use the average number of rooms as our x value and the average median income as our y value.\n",
    "\n",
    "Exercise 142: Creating a Scatter Plot for the Boston Housing Dataset\n",
    "In this exercise, you will create a scatter plot for our Boston housing dataset.\n",
    "\n",
    "The following steps will enable you to complete the exercise:\n",
    "\n",
    "Open a new Jupyter file and copy the dataset file into a separate folder.\n",
    "Import pandas and choose a variable to store the DataFrame and place the HousingData.csv file:\n",
    "import pandas as pd\n",
    "housing_df = pd.read_csv('HousingData.csv')\n",
    "Next, you need to import matplotlib as plt, as shown here:\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "Now you need to plot the scatter plot data, as shown in the following code snippet:\n",
    "x = housing_df['RM']\n",
    "y = housing_df['MEDV']\n",
    "plt.scatter(x, y) \n",
    "plt.show()\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.46: Scatter plot output of the dataset\n",
    "There is a definite positive association. This means that as the x value goes up, the y value goes up.\n",
    "\n",
    "In statistics, there is a particular concept to determine association; it's called correlation.\n",
    "\n",
    "\n",
    "COMPLETE & C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Correlation\n",
    "Correlation is a statistical measure between -1 and +1 that indicates how closely two variables are related. A correlation of -1 or +1 means that variables are completely dependent, and they fall in a perfectly straight line. A correlation of 0 indicates that an increase in one variable gives no information whatsoever about the other variable. Visually, this would be points all over the place. Correlations usually fall somewhere in the middle. For instance, a correlation of 0.75 represents a fairly strong relationship, whereas a correlation of 0.25 is a reasonably weak relationship. Positive correlations go up (meaning as x goes up, y goes up), and negative correlations go down.\n",
    "\n",
    "In the following exercise, you will find the correlation values from the Boston Housing dataset.\n",
    "\n",
    "Exercise 143: Correlation Values from the Dataset\n",
    "In this exercise, you will find the correlation values from the Boston Housing dataset. The following steps will enable you to complete the exercise:\n",
    "\n",
    "Open a new Jupyter Notebook and copy the dataset file into a separate folder where you will perform this exercise.\n",
    "Import pandas and choose a variable to store DataFrame and place the HousingData.csv file:\n",
    "import pandas as pd\n",
    "housing_df = pd.read_csv('HousingData.csv')\n",
    "Next, you need to import matplotlib as plt, as shown here:\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "Now, find the correlation value of the dataset, as shown in the following code snippet:\n",
    "housing_df.corr()\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.47: Correlation value output\n",
    "This tells us the exact correlation values. For instance, to see what variables are the most correlated with the Median Value Home, you can examine the values under the MEDV column. There, you will find that RM is the largest at 0.695360. But you also see a value of -0.735822 for LSTAT, which is the percentage of the lower status of the population. Seaborn provides a nice way to view correlations, called a heatmap. You can have a look at a heatmap in the following step.Now, you need to get the heatmap for the correlation values.\n",
    "Begin by importing the seaborn module as sns:\n",
    "import seaborn as sns\n",
    "# Set up seaborn dark grid\n",
    "sns.set()\n",
    "Now find the heatmap, as shown in the following code snippet:\n",
    "corr = housing_df.corr()\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr, xticklabels=corr.columns.values, \n",
    "yticklabels=corr.columns.values, cmap=\"Blues\", linewidths=1.25, alpha=0.8)\n",
    "plt.show()\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.48: Heatmap for the correlation values\n",
    "The darker the squares, the higher the correlation, and the lighter the squares, the lower the correlation. Now when examining the MEDV column, it's much easier to find the darkest square RM, and the lightest square LSTAT. You may have noticed that technically the MEDV square is the darkest. This has to be true because MEDV is perfectly correlated with itself. The same holds for each column along the diagonal.\n",
    "\n",
    "In this exercise, you were able to work with correlation values from the dataset and get a visual aid for the data output.\n",
    "\n",
    "In the next section, you will have a look at regression.\n",
    "\n",
    "\n",
    "COMPLETE & CONTINUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Regression\n",
    "Perhaps the most important addition to a scatter plot is the regression line. The idea of regression came from Sir Francis Galton, who measured the heights of the offspring of very tall and very short parents. The offspring were not taller or shorter than these parents on average, but rather closer to the mean height of all people. Sir Francis Galton used the term \"regression to the mean,\" meaning that the heights of the offspring were closer to the mean of their very tall or very short parents. The name stuck.\n",
    "\n",
    "In statistics, a regression line is a line that tries to fit the values of a scatter plot as closely as possible. Generally speaking, a good regression line has half of the points are above the line, and half of the points are below. The most popular regression line method is ordinary least squares, which tries to measure the error between prediction and true data by computing the sum of the square of the distance from each point to the corresponding prediction on the line and iteratively tries to reduce it.\n",
    "\n",
    "There are a variety of methods to compute and display regression lines using Python.\n",
    "\n",
    "Plotting a Regression Line\n",
    "To create a regression line of our Boston dataset, the following code steps need to be followed:\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.regplot(x,y)\n",
    "plt.show()\n",
    "You should get the following output:\n",
    "\n",
    "\n",
    "Figure 10.49: Example output of a regression line\n",
    "You may wonder about the shaded part of the line. It represents a 95% confidence interval, meaning that Python is 95% confident that the actual regression line falls within that range. Since the shaded area is fairly small in relation to the plot, this means that the regression line is reasonably accurate.\n",
    "\n",
    "The general idea behind regression lines is that they can be used to predict new y values from new x values. For instance, if there is an eight-room house, you can use regression to get an estimate of its value. You will use this general philosophy in a more sophisticated manner in course, Machine Learning, using the machine learning version of linear regression.\n",
    "\n",
    "Although this is not a course on statistics, you want to provide enough of an introduction so that you can analyze data on your own. In this respect, there is one more key piece to the regression that is worth sharing. It's the data about the line itself.\n",
    "\n",
    "StatsModel Regression Output\n",
    "You will import StatsModel and use its methods to print out a summary of the regression line:\n",
    "\n",
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(x)\n",
    "model = sm.OLS(y, X)\n",
    "est = model.fit()\n",
    "print(est.summary())\n",
    "The strangest part of the code is adding the constant. This is basically the y-intercept. When the constant is not added, the y-intercept is 0. In our case, it makes sense that the y-intercept would be 0; if there are 0 rooms, the house should have no value. In general, however, it's a good idea to keep a y-intercept, and it's the default choice of the preceding Seaborn graph. It's a good idea to try both methods and compare the results of the data. A comparative analysis will definitely improve your background in statistics:\n",
    "\n",
    "\n",
    "Figure 10.50: Summary of the regression line\n",
    "There's a lot of important information in this table. The first is the value of R^2 at 0.484. This suggests that 48% of the variance of the data can be explained by the regression line. The second is the coefficient constant of -34.6706. This is the y-intercept. The third is the RM coefficient of 9.1021. This suggests that for every one-bedroom increase, the value of the house increased by 9.102. (Keep in mind that this dataset is from 1980.)\n",
    "\n",
    "The standard error suggests how far off the actual values are from the line on average, and the numbers underneath the [0.025 0.975] column give the 95% Confidence Interval of the value, meaning statsmodel is 95% confident that the true increase in the value of the average house for every one-bedroom increase is between 8.279 and 9.925.\n",
    "\n",
    "Additional Models\n",
    "There's a great deal of data analysis in Python — far more than you can adequately cover in an introductory text. Thus far, you have covered histograms and scatter plots in considerable detail. Two additional types of plots will be highlighted before moving on to machine learning.\n",
    "\n",
    "\n",
    "COMPLETE &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Box Plots\n",
    "A box plot provides a nice visual of the mean, median, quartiles, and outliers of a given column of data.\n",
    "\n",
    "In this exercise, you will create box plots using the Boston Housing dataset. The following steps will enable you to complete the exercise:\n",
    "\n",
    "Open a new Jupyter Notebook and copy the dataset file within a separate folder where you will perform this exercise.\n",
    "Import pandas and choose a variable to store the DataFrame and place the HousingData.csv file:\n",
    "import pandas as pd\n",
    "housing_df = pd.read_csv('HousingData.csv')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# Set up seaborn dark grid\n",
    "sns.set()\n",
    "Now, enter the following code in the code snippet to create a box plot:\n",
    "x = housing_df['RM']\n",
    "y = housing_df['MEDV']\n",
    "plt.boxplot(x)\n",
    "plt.show()\n",
    "You should get the following output:\n",
    "\n",
    "Figure 10.51: Box plot output\n",
    "Note that the open circles are considered outliers. The orange bar in the middle is the median, and the bars at the end of the black box are the 25th and 75th percentiles, or the 1st and 3rd quartiles, respectively. The end bars represent the quartiles plus or minus 1.5 times the interquartile range. The value of 1.5 times the interquartile range is a standard limit in statistics used to define outliers. In this exercise, you created a box plot graph to represent the graph.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Analysis to Find the Outliers in Pay versus the Salary Report in the UK Statistics Dataset\n",
    "You are working as a data scientist, and you come across a government dataset that seems really interesting with regard to pay. But as the dataset values are cluttered, you need to use visual data analysis to study the data and come to a conclusion to understand whether there are any outliers in the dataset.\n",
    "\n",
    "In this activity, you will be performing visual data analysis using histograms and scatter plots and creating a regression line to come to a certain conclusion.\n",
    "\n",
    "Follow these steps to complete this activity:\n",
    "\n",
    "First, you need to copy the UKStatistics.csv dataset file into a specific folder.\n",
    "Now, import the necessary data visualization packages.\n",
    "View the dataset file and find the number of rows and columns in the dataset file.\n",
    "Plot the histogram for Actual Pay Floor (£).\n",
    "Plot the scatter plot using the values for x as Salary Cost of Reports (£) and y as Actual Pay Floor (£).\n",
    "Now get the box plot for the x and y values, as shown in Step 5.\n",
    "Note: UKStatistics.csv is available for download on GitHub here.\n",
    "\n",
    "More information on the UKStatistics dataset can be found here.\n",
    "\n",
    "Here is the expected output with the outliers in the box plot:\n",
    "\n",
    "Figure 10.53: The expected output with the outliers in the x and y data\n",
    "Note: If you get stuck, remember that you can find full solution files at the end of the course. \n",
    "\n",
    "\n",
    "COMPLETE &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Violin Plots\n",
    "A violin plot is a different type of plot that conveys similar information:\n",
    "\n",
    "plt.violinplot(x)\n",
    "plt.show()\n",
    "You should get the following output:\n",
    "\n",
    "\n",
    "Figure 10.52: Violin plot output\n",
    "In the violin plot, the upper and lower bars define the minimum and maximum values, and the width of the plot indicates how many rows contain that particular value. The difference between the violin plot and the box plot is that the violin plot shows the overall distribution of the data.\n",
    "\n",
    "Now, you will have a look at an activity to see whether you are able to implement the concepts covered in this course.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary\n",
    "You began our introduction to data analysis with NumPy, Python's incredibly fast library for handling massive matrix computations. Next, you learned about the fundamentals of pandas, Python's library for handling DataFrames.\n",
    "Taken together, you used NumPy and pandas to analyze the Boston Housing dataset, \n",
    "which included descriptive statistical methods and Matplotlib and Seaborn's graphical libraries.\n",
    "Along the way, you learned about fundamental statistical concepts, including the mean, standard deviation, median, quartiles, correlation, skewed data, and outliers. You also learned about advanced methods for creating clean, clearly labeled, publishable graphs.\n",
    "\n",
    "In course, Machine Learning, you will come across interesting machine learning concepts \n",
    "as regression, different types of classifications, decision trees. You will use Python to build efficient machine learning models and predict new results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
